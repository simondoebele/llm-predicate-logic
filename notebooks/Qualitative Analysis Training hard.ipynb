{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative Analysis Fewshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from llm_formalization.Parser import parse_LLM_output\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "from Parser import parse_LLM_output\n",
    "from evaluate_tasks import *\n",
    "import json\n",
    "from nltk.sem.logic import *\n",
    "import nltk\n",
    "from nltk.sem.logic import LogicParser, Expression\n",
    "from nltk.sem.evaluate import Valuation, Model\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir('../results/training-hard-eval/') if f.endswith('.json')]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_files = [f for f in files if \"Falcon\" in f]\n",
    "falcon_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_files = [f for f in files if \"Llama\" in f]\n",
    "llama_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orca_files = [f for f in files if \"orca\" in f]\n",
    "orca_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wizard_files = [f for f in files if \"wizard\" in f]\n",
    "wizard_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%Auf hard task 1:\n",
    "%- Falcon trained on t1t2t3 75.4                 (DONE)\n",
    "%- Falcon trained on t1 84.1                     (DONE)\n",
    "% Llama trained on t1!! 98.5                     (DONE)\n",
    "%Auf hard task 2:             \n",
    "%- Falcon trained on task 2 -> hard t2 89.2 *    (DONE)\n",
    "%- Llama trained on task 2 -> hard t2 89.6       (DONE)\n",
    "%- orca t1t2t3 -> hard t2 89.6                   (DONE)\n",
    "%- orca trained on task 2 -> hard t2 84.8        (DONE)\n",
    "%- wizard trained on task 2 -> hard t2 89.6      (DONE)\n",
    "%- wizard trained on task 3 -> hard t2 88.1 *    (DONE)\n",
    "%Auf hard task 3:\n",
    "%- llama trained on t3 -> 86.7                   (DONE)\n",
    "%- orca trained on t3 -> 90.1                    (DONE)\n",
    "%- wizard trained on t3 -> 78.3                  (DONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_tot1t2t3_t1 = pd.read_json('../results/training-hard-eval/' + falcon_files[3])\n",
    "falcon_tot1t2t3_t1.loc[falcon_tot1t2t3_t1['Correct'] == True].sample(n = 30, random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_tot1_t1 = pd.read_json('../results/training-hard-eval/' + falcon_files[2])\n",
    "falcon_tot1_t1.loc[falcon_tot1_t1['Correct'] == True].sample(n = 30, random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tot1t2t3_t1 = pd.read_json('../results/training-hard-eval/' + llama_files[2])\n",
    "llama_tot1t2t3_t1.loc[llama_tot1t2t3_t1['Correct'] == True].sample(n = 30, random_state = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Falcon trained on task 2, \n",
    "Orca trained on all tasks and \n",
    "wizard trained on task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_tot2_t2 = pd.read_json('../results/training-hard-eval/' + falcon_files[4])\n",
    "falcon_tot2_t2.loc[falcon_tot2_t2['Correct'] == True].sample(n = 30, random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orca_tot1t2t3_t2 = pd.read_json('../results/training-hard-eval/' + orca_files[2])\n",
    "orca_tot1t2t3_t2.loc[orca_tot1t2t3_t2['Correct'] == True].sample(n = 30, random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wizard_tot3_t2 = pd.read_json('../results/training-hard-eval/' + wizard_files[2])\n",
    "wizard_tot3_t2.loc[wizard_tot3_t2['Correct'] == True].sample(n = 30, random_state = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- orca\n",
    "- llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orca_tot3_t3 = pd.read_json('../results/training-hard-eval/' + orca_files[0])\n",
    "orca_tot3_t3.loc[orca_tot3_t3['Correct'] == True].sample(n = 30, random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tot3_t3 = pd.read_json('../results/training-hard-eval/' + llama_files[0])\n",
    "llama_tot3_t3.loc[llama_tot3_t3['Correct'] == True].sample(n = 30, random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
