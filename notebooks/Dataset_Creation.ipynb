{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUwWdisbYlzb"
      },
      "source": [
        "# Dataset Generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import Dataset creation functions from Dataset.py\n",
        "from Predicate_Logic_Dataset import *\n",
        "import Predicate_Logic_Dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fSr0pLjZEz10"
      },
      "source": [
        "## What is our dataset made up of?\n",
        "\n",
        "- Formula (in predicate logic)\n",
        "- Keys (a mapping between constants, predicates to the world (we are talking about) (i.e. predicates are adjectives / constants are names of persons - a very small subset of our language is therefore used.))\n",
        "- Model (sentences in natural language that describe our world: we have names of persons and adjectives that either describe or don't describe those persons)\n",
        "- Satisfied (whether the formula is satisfied given our model (and key mapping) -> we get this via the nltk theorem prover)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4q08u8Y197N"
      },
      "source": [
        "Before creating the input, we first need to agree on our language (the words to be used)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXxsGwXIMRbe"
      },
      "outputs": [],
      "source": [
        "male_names = ['Liam', 'Noah', 'Oliver', 'William', 'Elijah', 'James', 'Benjamin', 'Lucas', 'Mason', 'Ethan', 'Alexander', 'Henry', 'Jacob', 'Michael', 'Daniel', 'Logan', 'Jackson', 'Sebastian', 'Jack', 'Aiden', 'Owen', 'Samuel', 'Matthew', 'Joseph', 'Levi', 'Mateo', 'David', 'John', 'Wyatt', 'Carter', 'Julian', 'Luke', 'Grayson', 'Isaac', 'Jayden', 'Theodore', 'Gabriel', 'Anthony', 'Dylan', 'Leo', 'Lincoln', 'Jaxon', 'Asher', 'Christopher', 'Josiah', 'Andrew', 'Thomas', 'Joshua', 'Ezra', 'Hudson', 'Charles', 'Caleb', 'Isaiah', 'Ryan', 'Nathan', 'Adrian', 'Christian', 'Maverick', 'Colton', 'Elias', 'Aaron', 'Eli', 'Landon', 'Jonathan', 'Nolan', 'Hunter', 'Cameron', 'Connor', 'Santiago', 'Jeremiah', 'Ezekiel', 'Angel', 'Roman', 'Easton', 'Miles', 'Robert', 'Jameson', 'Nicholas', 'Greyson', 'Cooper', 'Ian', 'Carson', 'Axel', 'Jaxson', 'Dominic', 'Leonardo', 'Luca', 'Austin', 'Jordan', 'Adam', 'Xavier', 'Jose', 'Jace', 'Everett', 'Declan', 'Evan', 'Kayden', 'Parker', 'Wesley', 'Kai', 'Brayden', 'Bryson', 'Weston', 'Jason', 'Emmett', 'Sawyer', 'Silas', 'Bennett', 'Brooks', 'Micah', 'Damian', 'Harrison', 'Waylon', 'Ayden', 'Vincent', 'Ryder', 'Kingston', 'Rowan', 'George', 'Luis', 'Chase', 'Cole', 'Nathaniel', 'Zachary', 'Ashton', 'Braxton', 'Gavin', 'Tyler', 'Diego', 'Bentley', 'Amir', 'Beau', 'Gael', 'Carlos', 'Ryker', 'Jasper', 'Max', 'Juan', 'Ivan', 'Brandon', 'Jonah', 'Giovanni', 'Kaiden', 'Myles', 'Calvin', 'Lorenzo', 'Maxwell', 'Jayce', 'Kevin', 'Legend', 'Tristan', 'Jesus', 'Jude', 'Zion', 'Justin', 'Maddox', 'Abel', 'King', 'Camden', 'Elliott', 'Malachi', 'Milo', 'Emmanuel', 'Karter', 'Rhett', 'Alex', 'August', 'River', 'Xander', 'Antonio', 'Brody', 'Finn', 'Elliot', 'Dean', 'Emiliano', 'Eric', 'Miguel', 'Arthur', 'Matteo', 'Graham', 'Alan', 'Nicolas', 'Blake', 'Thiago', 'Adriel', 'Victor', 'Joel', 'Timothy', 'Hayden', 'Judah', 'Abraham', 'Edward', 'Messiah', 'Zayden', 'Theo', 'Tucker', 'Grant', 'Richard', 'Alejandro', 'Steven', 'Jesse', 'Dawson', 'Bryce', 'Avery', 'Oscar', 'Patrick', 'Archer', 'Barrett', 'Leon', 'Colt', 'Charlie', 'Peter', 'Kaleb', 'Lukas', 'Beckett', 'Jeremy', 'Preston', 'Enzo', 'Luka', 'Andres', 'Marcus', 'Felix', 'Mark', 'Ace', 'Brantley', 'Atlas', 'Remington', 'Maximus', 'Matias', 'Walker', 'Kyrie', 'Griffin', 'Kenneth', 'Israel', 'Javier', 'Kyler', 'Jax', 'Amari', 'Zane', 'Emilio', 'Knox', 'Adonis', 'Aidan', 'Kaden', 'Paul', 'Omar', 'BrianPeter', 'Louis', 'Caden', 'Maximiliano', 'Holden', 'Paxton', 'Nash', 'Bradley', 'Bryan', 'Simon', 'Phoenix', 'Lane', 'Josue', 'Colin', 'Rafael', 'Kyle', 'Riley', 'Jorge', 'Beckham', 'Cayden', 'Jaden', 'Emerson', 'Ronan', 'Karson', 'Arlo', 'Tobias', 'Brady', 'Clayton', 'Francisco', 'Zander', 'Erick', 'Walter', 'Daxton', 'Martin', 'Damien', 'Dallas', 'Cody', 'Chance', 'Jensen', 'Finley', 'Jett', 'Corbin', 'Kash', 'Reid', 'Kameron', 'Andre', 'Gunner', 'Jake', 'Hayes', 'Manuel', 'Prince', 'Bodhi', 'Cohen', 'Sean', 'Khalil', 'Hendrix', 'Derek', 'Cristian', 'Cruz', 'Kairo', 'Dante', 'Atticus', 'Killian', 'Stephen', 'Orion', 'Malakai', 'Ali', 'Eduardo', 'Fernando', 'Anderson', 'Angelo', 'Spencer', 'Gideon', 'Mario', 'Titus', 'Travis', 'Rylan', 'Kayson', 'Ricardo', 'Tanner', 'Malcolm', 'Raymond', 'Odin', 'Cesar', 'Lennox', 'Joaquin', 'Kane', 'Wade', 'Muhammad', 'Iker', 'Jaylen', 'Crew', 'Zayn', 'Hector', 'Ellis', 'Leonel', 'Cairo', 'Garrett', 'Romeo', 'Dakota', 'Edwin', 'Warren', 'Julius', 'Major', 'Donovan', 'Caiden', 'Tyson', 'Nico', 'Sergio', 'Nasir', 'Rory', 'Devin', 'Jaiden', 'Jared', 'Kason', 'Malik', 'Jeffrey', 'Ismael', 'Elian', 'Marshall', 'Lawson', 'Desmond', 'Winston', 'Nehemiah', 'Ari', 'Conner', 'Jay', 'Kade', 'Andy', 'Johnny', 'Jayceon', 'Marco', 'Seth', 'Ibrahim', 'Raiden', 'Collin', 'Edgar', 'Erik', 'Troy', 'Clark', 'Jaxton', 'Johnathan', 'Gregory', 'Russell', 'Royce', 'Fabian', 'Ezequiel', 'Noel', 'Pablo', 'Cade', 'Pedro', 'Sullivan', 'Trevor', 'Reed', 'Quinn', 'Frank', 'Harvey', 'Princeton', 'Zayne', 'Matthias', 'Conor', 'Sterling', 'Dax', 'Grady', 'Cyrus', 'Gage', 'Leland', 'Solomon', 'Emanuel', 'Niko', 'Ruben', 'Kasen', 'Mathias', 'Kashton', 'Franklin', 'Remy', 'Shane', 'Kendrick', 'Shawn', 'Otto', 'Armani', 'Keegan', 'Finnegan', 'Memphis', 'Bowen', 'Dominick', 'Kolton', 'Jamison', 'Allen', 'Philip', 'Tate', 'Peyton', 'Jase', 'Oakley', 'Rhys', 'Kyson', 'Adan', 'Esteban', 'Dalton', 'Gianni', 'Callum', 'Sage', 'Alexis', 'Milan', 'Moises', 'Jonas', 'Uriel', 'Colson', 'Marcos', 'Zaiden', 'Hank', 'Damon', 'Hugo', 'Ronin', 'Royal', 'Kamden', 'Dexter', 'Luciano', 'Alonzo', 'Augustus', 'Kamari', 'Eden', 'Roberto', 'Baker', 'Bruce', 'Kian', 'Albert', 'Frederick', 'Mohamed', 'Abram', 'Omari', 'Porter', 'Enrique', 'Alijah', 'Francis', 'Leonidas', 'Zachariah', 'Landen', 'Wilder', 'Apollo', 'Santino', 'Tatum', 'Pierce', 'Forrest', 'Corey', 'Derrick', 'Isaias', 'Kaison', 'Kieran', 'Arjun', 'Gunnar', 'Rocco', 'Emmitt', 'Abdiel', 'Braylen', 'Maximilian', 'Skyler', 'Phillip', 'Benson', 'Cannon', 'Deacon', 'Dorian', 'Asa', 'Moses', 'Ayaan', 'Jayson', 'Raul', 'Briggs', 'Armando', 'Nikolai', 'Cassius', 'Drew', 'Rodrigo', 'Raphael', 'Danny', 'Conrad', 'Moshe', 'Zyaire', 'Julio', 'Casey', 'Ronald', 'Scott', 'Callan', 'Roland', 'Saul', 'Jalen', 'Brycen', 'Ryland', 'Lawrence', 'Davis', 'Rowen', 'Zain', 'Ermias', 'Jaime', 'Duke', 'Stetson', 'Alec', 'Yusuf', 'Case', 'Trenton', 'Callen', 'Ariel', 'Jasiah', 'Soren', 'Dennis', 'Donald', 'Keith', 'Izaiah', 'Lewis', 'Kylan', 'Kobe', 'Makai', 'Rayan', 'Ford', 'Zaire', 'Landyn', 'Roy', 'Bo', 'Chris', 'Jamari', 'Ares', 'Mohammad', 'Darius', 'Drake', 'Tripp', 'Marcelo', 'Samson', 'Dustin', 'Layton', 'Gerardo', 'Johan', 'Kaysen', 'Keaton', 'Reece', 'Chandler', 'Lucca', 'Mack', 'Baylor', 'Kannon', 'Marvin', 'Huxley', 'Nixon', 'Tony', 'Cason', 'Mauricio', 'Quentin', 'Edison', 'Quincy', 'Ahmed', 'Finnley', 'Justice', 'Taylor', 'Gustavo', 'Brock', 'Ahmad', 'Kyree', 'Arturo', 'Nikolas', 'Boston', 'Sincere', 'Alessandro', 'Braylon', 'Colby', 'Leonard', 'Ridge', 'Trey', 'Aden', 'Leandro', 'Sam', 'Uriah', 'Ty', 'Sylas', 'Axton', 'Issac', 'Fletcher', 'Julien', 'Wells', 'Alden', 'Vihaan', 'Jamir', 'Valentino', 'Shepherd', 'Keanu', 'Hezekiah', 'Lionel', 'Kohen', 'Zaid', 'Alberto', 'Neil', 'Denver', 'Aarav', 'Brendan', 'Dillon', 'Koda', 'Sutton', 'Kingsley', 'Sonny', 'Alfredo', 'Wilson', 'Harry', 'Jaziel', 'Salvador', 'Cullen', 'Hamza', 'Dariel', 'Rex', 'Zeke', 'Mohammed', 'Nelson', 'Boone', 'Ricky', 'Santana', 'Cayson', 'Lance', 'Raylan', 'Lucian', 'Eliel', 'Alvin', 'Jagger', 'Braden', 'Curtis', 'Mathew', 'Jimmy', 'Kareem', 'Archie', 'Amos', 'Quinton', 'Yosef', 'Bodie', 'Jerry', 'Langston', 'Axl', 'Stanley', 'Clay', 'Douglas', 'Layne', 'Titan', 'Tomas', 'Houston', 'Darren', 'Lachlan', 'Kase', 'Korbin', 'Leighton', 'Joziah', 'Samir', 'Watson', 'Colten', 'Roger', 'Shiloh', 'Tommy', 'Mitchell', 'Azariah', 'Noe', 'Talon', 'Deandre', 'Lochlan', 'Joe', 'Carmelo', 'Otis', 'Randy', 'Byron', 'Chaim', 'Lennon', 'Devon', 'Nathanael', 'Bruno', 'Aryan', 'Flynn', 'Vicente', 'Brixton', 'Kyro', 'Brennan', 'Casen', 'Kenzo', 'Orlando', 'Castiel', 'Rayden', 'Ben', 'Grey', 'Jedidiah', 'Tadeo', 'Morgan', 'Augustine', 'Mekhi', 'Abdullah', 'Ramon', 'Saint', 'Emery', 'Maurice', 'Jefferson', 'Maximo', 'Koa', 'Ray', 'Jamie', 'Eddie', 'Guillermo', 'Onyx', 'Thaddeus', 'Wayne', 'Hassan', 'Alonso', 'Dash', 'Elisha', 'Jaxxon', 'Rohan', 'Carl', 'Kelvin', 'Jon', 'Larry', 'Reese', 'Aldo', 'Marcel', 'Melvin', 'Yousef', 'Aron', 'Kace', 'Vincenzo', 'Kellan', 'Miller', 'Jakob', 'Reign', 'Kellen', 'Kristopher', 'Ernesto', 'Briar', 'Gary', 'Trace', 'Joey', 'Clyde', 'Enoch', 'Jaxx', 'Crosby', 'Magnus', 'Fisher', 'Jadiel', 'Bronson', 'Eugene', 'Lee', 'Brecken', 'Atreus', 'Madden', 'Khari', 'Caspian', 'Ishaan', 'Kristian', 'Westley', 'Hugh', 'Kamryn', 'Musa', 'Rey', 'Thatcher', 'Alfred', 'Emory', 'Kye', 'Reyansh', 'Yahir', 'Cain', 'Mordechai', 'Zayd', 'Demetrius', 'Harley', 'Felipe', 'Louie', 'Branson', 'Graysen', 'Allan', 'Kole', 'Harold', 'Alvaro', 'Harlan', 'Amias', 'Brett', 'Khalid', 'Misael', 'Westin', 'Zechariah', 'Aydin', 'Kaiser', 'Lian', 'Bryant', 'Junior', 'Legacy', 'Ulises', 'Bellamy', 'Brayan', 'Kody', 'Ledger', 'Eliseo', 'Gordon', 'London', 'Rocky', 'Valentin', 'Terry', 'Damari', 'Trent', 'Bentlee', 'Canaan', 'Gatlin', 'Kiaan', 'Franco', 'Eithan', 'Idris', 'Krew', 'Yehuda', 'Marlon', 'Rodney', 'Creed', 'Salvatore', 'Stefan', 'Tristen', 'Adrien', 'Jamal', 'Judson', 'Camilo', 'Kenny', 'Nova', 'Robin', 'Rudy', 'Van', 'Bjorn', 'Brodie', 'Mac', 'Jacoby', 'Sekani', 'Vivaan', 'Blaine', 'Ira', 'Ameer', 'Dominik', 'Alaric', 'Dane', 'Jeremias', 'Kyng', 'Reginald', 'Bobby', 'Kabir', 'Jairo', 'Alexzander', 'Benicio', 'Vance', 'Wallace', 'Zavier', 'Billy', 'Callahan', 'Dakari', 'Gerald', 'Turner', 'Bear', 'Jabari', 'Cory', 'Fox', 'Harlem', 'Jakari', 'Jeffery', 'Maxton', 'Ronnie', 'Yisroel', 'Zakai', 'Bridger', 'Remi', 'Arian', 'Blaze', 'Forest', 'Genesis', 'Jerome', 'Reuben', 'Wesson', 'Anders', 'Banks', 'Calum', 'Dayton', 'Kylen', 'Dangelo', 'Emir', 'Malakhi', 'Salem', 'Blaise', 'Tru', 'Boden', 'Kolten', 'Kylo', 'Aries', 'Henrik', 'Kalel', 'Landry', 'Marcellus', 'Zahir', 'Lyle', 'Dario', 'Rene', 'Terrance', 'Xzavier', 'Alfonso', 'Darian', 'Kylian', 'Maison', 'Foster', 'Keenan', 'Yahya', 'Heath', 'Javion', 'Jericho', 'Aziel', 'Darwin', 'Marquis', 'Mylo', 'Ambrose', 'Anakin', 'Jordy', 'Juelz', 'Toby', 'Yael', 'Azrael', 'Brentley', 'Tristian', 'Bode', 'Jovanni', 'Santos', 'Alistair', 'Braydon', 'Kamdyn', 'Marc', 'Mayson', 'Niklaus', 'Simeon', 'Colter', 'Davion', 'Leroy', 'Ayan', 'Dilan', 'Ephraim', 'Anson', 'Merrick', 'Wes', 'Will', 'Jaxen', 'Maxim', 'Howard', 'Jad', 'Jesiah', 'Ignacio', 'Zyon', 'Ahmir', 'Jair', 'Mustafa', 'Jermaine', 'Yadiel', 'Aayan', 'Dhruv', 'Seven']\n",
        "\n",
        "female_names = ['Olivia', 'Emma', 'Ava', 'Sophia', 'Isabella', 'Charlotte', 'Amelia', 'Mia', 'Harper', 'Evelyn', 'Abigail', 'Emily', 'Ella', 'Elizabeth', 'Camila', 'Luna', 'Sofia', 'Avery', 'Mila', 'Aria', 'Scarlett', 'Penelope', 'Layla', 'Chloe', 'Victoria', 'Madison', 'Eleanor', 'Grace', 'Nora', 'Riley', 'Zoey', 'Hannah', 'Hazel', 'Lily', 'Ellie', 'Violet', 'Lillian', 'Zoe', 'Stella', 'Aurora', 'Natalie', 'Emilia', 'Everly', 'Leah', 'Aubrey', 'Willow', 'Addison', 'Lucy', 'Audrey', 'Bella', 'Nova', 'Brooklyn', 'Paisley', 'Savannah', 'Claire', 'Skylar', 'Isla', 'Genesis', 'Naomi', 'Elena', 'Caroline', 'Eliana', 'Anna', 'Maya', 'Valentina', 'Ruby', 'Kennedy', 'Ivy', 'Ariana', 'Aaliyah', 'Cora', 'Madelyn', 'Alice', 'Kinsley', 'Hailey', 'Gabriella', 'Allison', 'Gianna', 'Serenity', 'Samantha', 'Sarah', 'Autumn', 'Quinn', 'Eva', 'Piper', 'Sophie', 'Sadie', 'Delilah', 'Josephine', 'Nevaeh', 'Adeline', 'Arya', 'Emery', 'Lydia', 'Clara', 'Vivian', 'Madeline', 'Peyton', 'Julia', 'Rylee', 'Brielle', 'Reagan', 'Natalia', 'Jade', 'Athena', 'Maria', 'Leilani', 'Everleigh', 'Liliana', 'Melanie', 'Mackenzie', 'Hadley', 'Raelynn', 'Kaylee', 'Rose', 'Arianna', 'Isabelle', 'Melody', 'Eliza', 'Lyla', 'Katherine', 'Aubree', 'Adalynn', 'Kylie', 'Faith', 'Mary', 'Margaret', 'Ximena', 'Iris', 'Alexandra', 'Jasmine', 'Charlie', 'Amaya', 'Taylor', 'Isabel', 'Ashley', 'Khloe', 'Ryleigh', 'Alexa', 'Amara', 'Valeria', 'Andrea', 'Parker', 'Norah', 'Eden', 'Elliana', 'Brianna', 'Emersyn', 'Valerie', 'Anastasia', 'Eloise', 'Emerson', 'Cecilia', 'Remi', 'Josie', 'Alina', 'Reese', 'Bailey', 'Lucia', 'Adalyn', 'Molly', 'Ayla', 'Sara', 'Daisy', 'London', 'Jordyn', 'Esther', 'Genevieve', 'Harmony', 'Annabelle', 'Alyssa', 'Ariel', 'Aliyah', 'Londyn', 'Juliana', 'Morgan', 'Summer', 'Juliette', 'Trinity', 'Callie', 'Sienna', 'Blakely', 'Alaia', 'Kayla', 'Teagan', 'Alaina', 'Brynlee', 'Finley', 'Catalina', 'Sloane', 'Rachel', 'Lilly', 'Ember', 'Kimberly', 'Juniper', 'Sydney', 'Arabella', 'Gemma', 'Jocelyn', 'Freya', 'June', 'Lauren', 'Amy', 'Presley', 'Georgia', 'Journee', 'Elise', 'Rosalie', 'Ada', 'Laila', 'Brooke', 'Diana', 'Olive', 'River', 'Payton', 'Ariella', 'Daniela', 'Raegan', 'Alayna', 'Gracie', 'Mya', 'Blake', 'Noelle', 'Ana', 'Leila', 'Paige', 'Lila', 'Nicole', 'Rowan', 'Hope', 'Ruth', 'Alana', 'Selena', 'Marley', 'Kamila', 'Alexis', 'Mckenzie', 'Zara', 'Millie', 'Magnolia', 'Kali', 'Kehlani', 'Catherine', 'Maeve', 'Adelyn', 'Sawyer', 'Elsie', 'Lola', 'Jayla', 'Adriana', 'Journey', 'Vera', 'Aspen', 'Joanna', 'Alivia', 'Angela', 'Dakota', 'Camille', 'Nyla', 'Tessa', 'Brooklynn', 'Malia', 'Makayla', 'Rebecca', 'Fiona', 'Mariana', 'Lena', 'Julianna', 'Vanessa', 'Juliet', 'Camilla', 'Kendall', 'Harley', 'Cali', 'Evangeline', 'Mariah', 'Jane', 'Zuri', 'Elaina', 'Sage', 'Amira', 'Adaline', 'Lia', 'Charlee', 'Delaney', 'Lilah', 'Miriam', 'Angelina', 'Mckenna', 'Aniyah', 'Phoebe', 'Michelle', 'Thea', 'Hayden', 'Maggie', 'Lucille', 'Amiyah', 'Annie', 'Alexandria', 'Myla', 'Vivienne', 'Kiara', 'Alani', 'Margot', 'Adelaide', 'Briella', 'Brynn', 'Saylor', 'Destiny', 'Amari', 'Evelynn', 'Haven', 'Phoenix', 'Izabella', 'Kaia', 'Lilliana', 'Harlow', 'Alessandra', 'Madilyn', 'Nina', 'Logan', 'Adelynn', 'Amina', 'Kate', 'Fatima', 'Samara', 'Winter', 'Giselle', 'Evie', 'Arielle', 'Jessica', 'Talia', 'Leia', 'Gabriela', 'Gracelyn', 'Lexi', 'Laura', 'Makenzie', 'Melissa', 'Royalty', 'Rylie', 'Raelyn', 'Gabrielle', 'Paris', 'Daleyza', 'Joy', 'Maisie', 'Oakley', 'Ariyah', 'Kailani', 'Alayah', 'Stephanie', 'Amora', 'Willa', 'Gracelynn', 'Elle', 'Keira', 'Tatum', 'Veronica', 'Milani', 'Felicity', 'Paislee', 'Allie', 'Nylah', 'Ariah', 'Cassidy', 'Lyric', 'Madeleine', 'Miracle', 'Gwendolyn', 'Octavia', 'Dahlia', 'Heidi', 'Celeste', 'Remington', 'Makenna', 'Everlee', 'Scarlet', 'Esmeralda', 'Maci', 'Lainey', 'Jacqueline', 'Kira', 'Lana', 'Brinley', 'Demi', 'Ophelia', 'Lennon', 'Reign', 'Bristol', 'Sabrina', 'Alaya', 'Jennifer', 'Kenzie', 'Angel', 'Luciana', 'Anaya', 'Hallie', 'Ryan', 'Camryn', 'Kinley', 'Daniella', 'Lilith', 'Blair', 'Amanda', 'Collins', 'Jordan', 'Maliyah', 'Rosemary', 'Cataleya', 'Kaylani', 'Gia', 'Alison', 'Leighton', 'Nadia', 'Sutton', 'Carolina', 'Skye', 'Alicia', 'Regina', 'Viviana', 'Yaretzi', 'Heaven', 'Serena', 'Raven', 'Emely', 'Carmen', 'Wren', 'Helen', 'Charleigh', 'Danielle', 'Daphne', 'Esme', 'Nayeli', 'Maddison', 'Sarai', 'Dylan', 'Frances', 'Elisa', 'Mabel', 'Skyler', 'Jenna', 'Emelia', 'Kaitlyn', 'Miranda', 'Marlee', 'Matilda', 'Selah', 'Jolene', 'Wynter', 'Hattie', 'Bianca', 'Haley', 'Lorelei', 'Mira', 'Braelynn', 'Annalise', 'Madelynn', 'Katie', 'Palmer', 'Aylin', 'Elliott', 'Kyla', 'Rory', 'Avianna', 'Liana', 'Shiloh', 'Kalani', 'Jada', 'Kelsey', 'Elianna', 'Jimena', 'Kora', 'Kamryn', 'Ainsley', 'Averie', 'Kensley', 'Helena', 'Holly', 'Emory', 'Macie', 'Amber', 'Zariah', 'Erin', 'Eve', 'Kathryn', 'Renata', 'Kayleigh', 'Emmy', 'Celine', 'Francesca', 'Fernanda', 'April', 'Shelby', 'Poppy', 'Colette', 'Meadow', 'Nia', 'Sierra', 'Cheyenne', 'Edith', 'Oaklynn', 'Kennedi', 'Abby', 'Danna', 'Jazlyn', 'Alessia', 'Mikayla', 'Alondra', 'Addilyn', 'Leona', 'Mckinley', 'Carter', 'Maren', 'Sylvia', 'Alejandra', 'Ariya', 'Astrid', 'Adrianna', 'Charli', 'Imani', 'Maryam', 'Christina', 'Stevie', 'Maia', 'Adelina', 'Dream', 'Aisha', 'Alanna', 'Itzel', 'Azalea', 'Katelyn', 'Kylee', 'Leslie', 'Madilynn', 'Myra', 'Virginia', 'Remy', 'Hanna', 'Aleah', 'Jaliyah', 'Antonella', 'Aviana', 'Cameron', 'Chelsea', 'Cecelia', 'Alia', 'Mae', 'Cadence', 'Emberly', 'Charley', 'Janelle', 'Mallory', 'Kaliyah', 'Elaine', 'Gloria', 'Jayleen', 'Lorelai', 'Malaysia', 'Bethany', 'Briana', 'Beatrice', 'Dorothy', 'Rosie', 'Jemma', 'Noa', 'Carly', 'Mariam', 'Anne', 'Karina', 'Emmalyn', 'Ivory', 'Ivanna', 'Jamie', 'Kara', 'Aitana', 'Jayda', 'Justice', 'Meredith', 'Briar', 'Skyla', 'Khaleesi', 'Dayana', 'Julieta', 'Katalina', 'Kendra', 'Oaklyn', 'Ashlyn', 'Armani', 'Jazmin', 'Kyra', 'Angelica', 'Zahra', 'Dallas', 'Johanna', 'Elliot', 'Macy', 'Monroe', 'Kimber', 'Henley', 'Ari', 'Karsyn', 'Lyanna', 'Lilian', 'Amalia', 'Nola', 'Dior', 'Aleena', 'Megan', 'Michaela', 'Amirah', 'Cassandra', 'Melany', 'Legacy', 'Reyna', 'Alma', 'Emmie', 'Melina', 'Siena', 'Priscilla', 'Ashlynn', 'Savanna', 'Sloan', 'Tiana', 'Aubrie', 'Coraline', 'Reina', 'Allyson', 'Kaydence', 'Sasha', 'Julie', 'Alexia', 'Irene', 'Marilyn', 'Greta', 'Braelyn', 'Emerie', 'Lylah', 'Nalani', 'Monica', 'Aileen', 'Lauryn', 'Anahi', 'Aurelia', 'Kassidy', 'Rayna', 'Romina', 'Lillie', 'Marie', 'Rosa', 'Saige', 'Bonnie', 'Kelly', 'Xiomara', 'Annabella', 'Avah', 'Lacey', 'Anya', 'Liberty', 'Karen', 'Mercy', 'Zelda', 'Baylee', 'Chaya', 'Kenna', 'Roselyn', 'Liv', 'Mara', 'Ensley', 'Malani', 'Malaya', 'Hadassah', 'Lyra', 'Adley', 'Galilea', 'Jaylah', 'Karla', 'Nala', 'Opal', 'Aliza', 'Milena', 'Ailani', 'Louisa', 'Mina', 'Kairi', 'Clementine', 'Louise', 'Maleah', 'Janiyah', 'Marina', 'Anika', 'Julissa', 'Bailee', 'Hayley', 'Jessie', 'Laney', 'Eileen', 'Faye', 'Kynlee', 'Tiffany', 'Lara', 'Angie', 'Joelle', 'Rhea', 'Calliope', 'Jazmine', 'Amani', 'Haylee', 'Aliana', 'Leyla', 'Jolie', 'Kinslee', 'Ryann', 'Simone', 'Milan', 'Lennox', 'Treasure', 'Alora', 'Ellis', 'Rebekah', 'Mikaela', 'Lina', 'Harmoni', 'Yareli', 'Giuliana', 'Lea', 'Harlee', 'Elyse', 'Frida', 'Blaire', 'Aya', 'Laurel', 'Meghan', 'Pearl', 'Zaylee', 'Alena', 'Holland', 'Bria', 'Rayne', 'Bridget', 'Zariyah', 'Kori', 'Frankie', 'Clarissa', 'Brylee', 'Davina', 'Rivka', 'Cynthia', 'Zaria', 'Madalyn', 'Paula', 'Salem', 'Amelie', 'Madisyn', 'Vienna', 'Haisley', 'Ainhoa', 'Journi', 'Karter', 'Oaklee', 'Livia', 'Miley', 'Adele', 'Amaia', 'Yara', 'Averi', 'Emmeline', 'Kyleigh', 'Princess', 'Penny', 'Sariyah', 'Amayah', 'Crystal', 'Keyla', 'Lilyana', 'Linda', 'Aniya', 'Marianna', 'Alaiya', 'Noemi', 'Chanel', 'Estella', 'Isabela', 'Jillian', 'Kallie', 'Ellianna', 'Elsa', 'Itzayana', 'Zora', 'Estelle', 'Chana', 'Raina', 'Royal', 'Sunny', 'Estrella', 'Martha', 'Ellen', 'Kailey', 'Maxine', 'Clare', 'Teresa', 'Annika', 'Kamilah', 'Azariah', 'Della', 'Addyson', 'Kai', 'Lilianna', 'Tinsley', 'Yaritza', 'Navy', 'Winnie', 'Andi', 'Kamiyah', 'Waverly', 'Sky', 'Amaris', 'Ramona', 'Saoirse', 'Hana', 'Judith', 'Halle', 'Laylah', 'Novalee', 'Jaycee', 'Zaniyah', 'Alianna', 'Paulina', 'Jayde', 'Thalia', 'Giovanna', 'Gwen', 'Iliana', 'Elora', 'Ezra', 'Kaylie', 'Braylee', 'Mavis', 'Ellison', 'Margo', 'Mylah', 'Paisleigh', 'Analia', 'August', 'Brittany', 'Kaisley', 'Belen', 'Promise', 'Amiya', 'Dalary', 'Veda', 'Alisson', 'Keilani', 'Oakleigh', 'Guadalupe', 'Leanna', 'Rosalyn', 'Selene', 'Theodora', 'Kamari', 'Anais', 'Elodie', 'Celia', 'Dani', 'Hunter', 'Indie', 'Kenia', 'Nellie', 'Belle', 'Kataleya', 'Lexie', 'Miah', 'Rylan', 'Sylvie', 'Valery', 'Addilynn', 'Dulce', 'Marissa', 'Meilani', 'Natasha', 'Jaylee', 'Kimora', 'Raquel', 'Scarlette', 'Aliya', 'Nataly', 'Whitney', 'Corinne', 'Denver', 'Nathalie', 'Kiera', 'Milana', 'Vada', 'Violeta', 'Luz', 'Addisyn', 'Casey', 'Deborah', 'Tori', 'Zainab', 'Erika', 'Jenesis', 'Avalynn', 'Nancy', 'Emmalynn', 'Hadlee', 'Heavenly', 'Aubrielle', 'Elisabeth', 'Salma', 'Adalee', 'Landry', 'Malayah', 'Novah', 'Egypt', 'Ayleen', 'Blessing', 'Elina', 'Joyce', 'Myah', 'Zoie', 'Christine', 'Jaelynn', 'Persephone', 'Chandler', 'Emmaline', 'Paloma', 'Harleigh', 'Noor', 'Paola', 'India', 'Madalynn', 'Rosalee', 'Florence', 'Maliah', 'Flora', 'Luella', 'Patricia', 'Whitley', 'Carolyn', 'Kathleen', 'Keily', 'Kiana', 'Tenley', 'Alyson', 'Barbara', 'Dana', 'Yasmin', 'Bexley', 'Micah', 'Tatiana', 'Arden', 'Aubriella', 'Lindsey', 'Emani', 'Hailee', 'Lisa', 'Sevyn', 'Fallon', 'Magdalena', 'Tinley', 'Halo', 'Lailah', 'Arlette', 'Ansley', 'Esperanza', 'Cleo', 'Aila', 'Emerald', 'Jaelyn', 'Karlee', 'Kaya', 'Ingrid', 'Jewel', 'Emilee', 'Giana', 'Paityn', 'Zola', 'Amoura', 'Renee', 'Ann', 'Berkley', 'Harriet', 'Queen', 'Sariah', 'Beatrix', 'Sandra', 'Alannah', 'Austyn', 'Freyja', 'Kaylin', 'Samira', 'Taliyah', 'Hadleigh', 'Kaiya', 'Robin', 'Luisa', 'Zendaya', 'Ariadne', 'Dixie']\n",
        "\n",
        "names = male_names + female_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YULhXCDbaOh3"
      },
      "source": [
        "Adjectives:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N12OmbUJaNYo"
      },
      "outputs": [],
      "source": [
        "# here we use a list of emotion adjectives:\n",
        "adjectives = [    \n",
        "    \"Accepting\",\"Accommodating\",\"Afraid\",\"Aggressive\",\"Agitated\",\"Alarmed\",\"Amazed\",\n",
        "    \"Amused\",\"Antagonistic\",\"Anxious\",\"Apathetic\",\"Apprehensive\",\"Arrogant\",\"Bothered\",\"Brave\",\"Calm\",\"Charming\",\n",
        "    \"Cheerful\",\"Competitive\",\"Confident\",\"Conscientious\",\"Conservative\",\"Creative\",\"Curious\",\n",
        "    \"Cynical\",\"Distracted\",\"Distraught\",\"Distressed\",\"Downcast\",\n",
        "    \"Earnest\",\"Gloomy\",\"Grouchy\",\"Guilty\",\"Heroic\",\"Hopeful\",\n",
        "    \"Hostile\",\"Humble\",\"Jovial\",\"Joyful\",\"Jumpy\",\"Mysterious\",\"Opinionated\",\n",
        "    \"Panicky\",\"Passionate\",\"Patient\",\"Quarrelsome\",\"Rational\",\"Reasonable\",\n",
        "    \"Sad\",\"Safe\",\"Scared\",\"Scornful\",\"Selfish\",\n",
        "    \"Thoughtful\",\"Tolerant\",\"Tranquil\",\"Treacherous\",\"Welcoming\",\"Witty\",\"Zealous\",\"Zany\",\n",
        "    \"Supportive\",\"Sardonic\",\"Secretive\",\"Bitter\",\n",
        "]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "c1dXLX8SGZLh"
      },
      "source": [
        "## (1) Formulas:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0UCuK0rtptii"
      },
      "source": [
        "Here, we define scope, i.e. which fragment of first-order predicate logic is to be considered:\n",
        "1. Both \"All quantified\" (forall) and \"existentially quantified\" (exists)? Or just one of the two? - Currently, we use both.\n",
        "2. How many variables do we allow for? - Currently, just one variable: x.\n",
        "3. What logical connectives are allowed? (not, implies, and, or, equivalence, ...) - Currently, we only allow for implications, the and operation and negation. However, we do not allow for combinations of implications and the and operation in one formula (for reasons of combinatorial explosion).\n",
        "4. How big of an \"n\" in n-ary predicates? - Currently, we only allow unary predicates.\n",
        "5. What recursion depth do we allow? - Currently, we only allow for recursion depth of 1 (e.g. phi(x) -> psi(x) cannot be \"developed\" into: F(x)&G(x) -> H(x)&I(x), where phi(x) = F(x)&G(x). The latter would require a recursion depth of 2.) This can be expanded later on.\n",
        "6. Note also what we do not allow: we do not allow simple atomic formulas, such as \"Fa\", where a is a constant and not a variable, as this is too trivial.\n",
        "\n",
        "This results in the following formulas to be considered:\n",
        "- all x. Fx → Gx\n",
        "- all x. -Fx → Gx\n",
        "- all x. Fx → !Gx\n",
        "- not (all x. Fx → Gx)\n",
        "- not (all x. -Fx → Gx)\n",
        "- not (all x. Fx → -Gx)\n",
        "- exists x. Fx → Gx\n",
        "- exists x. -Fx → Gx\n",
        "- exists x. Fx → -Gx\n",
        "- not (exists x. Fx → Gx)\n",
        "- not (exists x. -Fx → Gx)\n",
        "- not (exists x. Fx → -Gx)\n",
        "\n",
        "...(and so on, including the \"and\" operation where the implication is.)\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use the python nltk library to represent these formulas (see python script)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhJ8lt_4DbmV"
      },
      "outputs": [],
      "source": [
        "# Define the possible operators / logical connectives & quantifiers: the integer in each pair refers to the \"arity\" of the operator.\n",
        "#operators = [ImpExpression] # more: and, or, equivalence, ...\n",
        "#negation_operator = Tokens.NOT # special, because only takes one argument (instead of two, like the other operators)\n",
        "#quantifiers = [Tokens.ALL, Tokens.EXISTS]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lsps2XSDhQo"
      },
      "outputs": [],
      "source": [
        "# since we only have unary predicates, one variable will suffice.\n",
        "# generated as per the nltk.sem.logic formulation of Variables.\n",
        "variables = [Variable('x')]\n",
        "\n",
        "# The alphabet in capital letters (without X, Y, Z):\n",
        "possible_predicate_tokens = {'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W'}\n",
        "\n",
        "# Define the possible predicate tokens.\n",
        "unary_predicate_tokens = [Variable('F'), Variable('G')]\n",
        "binary_predicate_tokens = [] # When implementing this: it may not be allowed to put the same tokens here as for unary_predicate_tokens (, may it?)\n",
        "#...etc.\n",
        "\n",
        "negation = \"not\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAH3nPmf4bAb"
      },
      "outputs": [],
      "source": [
        "predicate_expressions, predicates, vars, operators, quantifiers = Predicate_Logic_Dataset.generate_unary_predicate_expressions(unary_predicate_tokens, variables)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoIHYbZotwDX",
        "outputId": "52924b08-592f-4215-ceac-b6641aa89004"
      },
      "outputs": [],
      "source": [
        "# print(predicate_expressions[0])\n",
        "# print(predicates[0])\n",
        "# print(vars[0])\n",
        "# print(operators[0])\n",
        "# print(quantifiers[0])\n",
        "\n",
        "# print(len(predicate_expressions))\n",
        "# print(len(predicates))\n",
        "# print(len(vars))\n",
        "# print(len(operators))\n",
        "# print(len(quantifiers))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tk2WCsbQ44BM",
        "outputId": "6dd17211-1083-4a03-c541-82605bc8aa50"
      },
      "outputs": [],
      "source": [
        "complex_formulas, predicates, vars, operators, quantifiers = Predicate_Logic_Dataset.generate_complex_predicate_expressions(predicate_expressions, predicates, vars, operators, quantifiers, 2)\n",
        "#for f in complex_formulas:\n",
        "#  print(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-Jot3Es44Vv"
      },
      "outputs": [],
      "source": [
        "formulas, predicates, vars, operators, quantifiers = Predicate_Logic_Dataset.generate_quantified(complex_formulas, predicates, vars, operators, quantifiers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dp1dEXf-zoOz",
        "outputId": "fc9f81ec-af0f-41d0-a5a6-a8ea44c6d8b3"
      },
      "outputs": [],
      "source": [
        "for f in formulas:\n",
        "  print(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(len(formulas))\n",
        "print(len(vars))\n",
        "print(len(predicates))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiQpc3R9lCED"
      },
      "source": [
        "## (2) Keys:\n",
        "\n",
        "Here we map the predicates and constants to actual words (from our dictionary of words, which are names and adjectives as seen above). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wecYm2SCzYeg"
      },
      "source": [
        "In the following, we have to decide how many constants we want each time.\n",
        "I decided it to be a random number between 1 and 10. They should be maximally 24 or 25 (depending on the size of the chosen alphabet for possible_constant_tokens)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAxECCFV4FqW"
      },
      "outputs": [],
      "source": [
        "# The alphabet in lower case letters (without u, v, w, x, y, z):\n",
        "possible_constant_tokens = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C49gDDBG8ohh",
        "outputId": "9c16bedb-191d-498f-e3db-2aa9333a6e3d"
      },
      "outputs": [],
      "source": [
        "predicate_mapping, constant_mapping, random_names, adjectives_subset = map_keys(predicates[0], adjectives, names, possible_constant_tokens)\n",
        "print(predicate_mapping)\n",
        "print(constant_mapping)\n",
        "print(random_names)\n",
        "print(adjectives_subset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9ClCKCHaJ8y"
      },
      "source": [
        "## (3) Model:\n",
        "\n",
        "A model consists of multiple sentences. Each sentence is made up of \"simple\" \"is\"- or \"is not\"-statements, e.g. Peter is anxious. I.e. in general: [Name] is (not) [Adjective]. We then generate all possible combinations of sentences given the predicates and names (randomly including negation or not).\n",
        "\n",
        "Each sentence comes with a corresponding nltk representation, resulting in both a model in natural language and a model that nltk can use later on, e.g. in order to decide whether a formula is satisfied given a model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentences, expressions = create_world_model(predicate_mapping, constant_mapping, negation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "expressions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#print(expressions[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_constant_mapping(constant_mapping):\n",
        "    return [(k, v) for k, v in constant_mapping.items()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "v = convert_constant_mapping(constant_mapping) + expressions\n",
        "v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# example from nltk website\n",
        "#from nltk.sem import Valuation, Model\n",
        "#v = [('adam', 'b1'), ('betty', 'g1'), ('fido', 'd1'), \n",
        "#     ('girl', set(['g1', 'g2'])), ('boy', set(['b1', 'b2'])), \n",
        "#     ('dog', set(['d1'])), \n",
        "#     ('love', set([('b1', 'g1'), ('b2', 'g2'), ('g1', 'b1'), ('g2', 'b1')]))\n",
        "#     ]\n",
        "#val = Valuation(v)\n",
        "#dom = val.domain\n",
        "#m = Model(dom, val)\n",
        "#g = nltk.sem.Assignment(dom)\n",
        "#sat = m.evaluate('all x.(boy(x) -> - girl(x))', g)\n",
        "#print(sat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "formulas[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "val = Valuation(v)\n",
        "dom = val.domain\n",
        "m = nltk.sem.evaluate.Model(dom, val)\n",
        "g = nltk.sem.Assignment(dom)\n",
        "sat = m.evaluate(str(formulas[0]), g)\n",
        "print(sat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TibCTMChcZwl"
      },
      "source": [
        "## The actual Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou2jafcHcdZf"
      },
      "source": [
        "Parameters that we need to decide upon:\n",
        "- number of model sentences: aka model size: i.e. the number of sentences the model can have (to be decided for each data point). This directly follows from the number of names we decided to include each time (see above)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "4sXom-B-dlID",
        "outputId": "4713fa14-dcf3-4415-ca02-77b2c59dd4b7"
      },
      "outputs": [],
      "source": [
        "# the data\n",
        "rows_list = [] \n",
        "\n",
        "max_num_names = 10\n",
        "\n",
        "# search for enough formula-model-combinations that are satisfied and unsatisfied, so that we get a balanced data set.\n",
        "num_satisfied = 50000\n",
        "num_unsatisfied = 50000\n",
        "count = 0\n",
        "while num_satisfied > 0 or num_unsatisfied > 0:\n",
        "  for i, formula in enumerate(formulas):\n",
        "    \n",
        "    # decide, whether we want to sample a true or false model.\n",
        "    label = random.choice([True, False])\n",
        "    \n",
        "    # map the keys\n",
        "    predicate_mapping, constant_mapping, random_names, adjectives_subset = map_keys(predicates[i], adjectives, names, possible_constant_tokens, max_num_names)\n",
        "\n",
        "    # sample a model\n",
        "    sentences, expressions = create_world_model(predicate_mapping, constant_mapping, negation)\n",
        "\n",
        "    # evaluate model and formula\n",
        "    v = convert_constant_mapping(constant_mapping) + expressions\n",
        "    val = Valuation(v)\n",
        "    dom = val.domain\n",
        "    m = Model(dom, val)\n",
        "    g = nltk.sem.Assignment(dom)\n",
        "    sat = m.evaluate(str(formula), g)\n",
        "\n",
        "    if sat == True and num_satisfied > 0:\n",
        "      num_satisfied -= 1\n",
        "      count += 1\n",
        "      # add to dataset (formula, keys, model_natural_language, model_nltk, sat, ...)\n",
        "      datapoint = {\"Formula\" :    str(formula), \n",
        "                   \"Predicates\" : predicates[i], \n",
        "                   \"Variables\":   vars[i], \n",
        "                   \"Operators\":   operators[i], \n",
        "                   \"Quantifiers\": str(quantifiers[i]), \n",
        "                   \"P_mapping\" :  predicate_mapping, \n",
        "                   \"C_mapping\" :  constant_mapping,\n",
        "                   \"Keys\" :       get_keys(predicate_mapping,constant_mapping),\n",
        "                   \"Names\" :      random_names,\n",
        "                   \"Adjectives\" : adjectives_subset,\n",
        "                   \"World Model\": sentences,\n",
        "                   \"Valuation\":   v,\n",
        "                   \"Satisfied\":   \"satisfied\"\n",
        "                  }\n",
        "      rows_list.append(datapoint)\n",
        "\n",
        "    if sat == False and num_unsatisfied > 0:\n",
        "      num_unsatisfied -= 1\n",
        "      count += 1\n",
        "      # add to dataset (formula, keys, model_natural_language, model_nltk, sat?)\n",
        "      datapoint = {\"Formula\" :    str(formula), \n",
        "                   \"Predicates\" : predicates[i], \n",
        "                   \"Variables\":   vars[i], \n",
        "                   \"Operators\":   operators[i], \n",
        "                   \"Quantifiers\": str(quantifiers[i]), \n",
        "                   \"P_mapping\" :  predicate_mapping, \n",
        "                   \"C_mapping\" :  constant_mapping,\n",
        "                   \"Keys\" :       get_keys(predicate_mapping,constant_mapping),\n",
        "                   \"Names\" :      random_names,\n",
        "                   \"Adjectives\" : adjectives_subset,\n",
        "                   \"World Model\": sentences,\n",
        "                   \"Valuation\":   v,\n",
        "                   \"Satisfied\":   \"unsatisfied\"\n",
        "                  }\n",
        "      rows_list.append(datapoint)\n",
        "    \n",
        "    print(count)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.DataFrame(rows_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_json('base_pred_logic_data.json')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task Dataset Creation: Converting this original dataset to task datasets"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following two different types of tasks are envisioned:\n",
        "\n",
        "Task 1:\n",
        "\n",
        "Input:\n",
        "“Here is a situation:\n",
        "{{ model }}\n",
        "Let us interpret predicates and names as follows: {{keys | join}}\n",
        "Provide a logical formula that is {{ satisfied }} given the above situation and interpretation (keys)!\n",
        "“\n",
        "Target: \n",
        "“ {{ formula }} “\n",
        "\n",
        "(Beachte: mehrere richtige Antworten möglich.\n",
        "Formel lösen lassen (per solver))\n",
        "\n",
        "\n",
        "Task 2:\n",
        "\n",
        "Input:\n",
        "“Consider the following formula:\n",
        "{{ formula }}\n",
        "describe a situation in which the formula is {{ satisfied }} and provide the keys! “\n",
        "Target: \n",
        "“situation:\n",
        "{{model | join}} keys: {{keys | join}}\n",
        "“\n",
        "\n",
        "Task 3:\n",
        "Input:\n",
        "\"\"Consider the following formula\"\n",
        "{{ formula }}\n",
        "Let us interpret predicates and names as follows: {{keys | join}}.\n",
        "Also, here is a situation:\n",
        "{{ model }}.\n",
        "Is the formula satisfied or not given the situation?\n",
        "\"\n",
        "Target: \n",
        "sat oder unsat (classification.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_json('base_pred_logic_data.json')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### TASK 3"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Input: Formula + Keys + World Model\n",
        "\n",
        "Output: true / false (sat / unsat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inputs = make_task3_inputs(df)\n",
        "targets = df['Satisfied'].tolist()\n",
        "\n",
        "d = {'Input':inputs,'Target':targets}\n",
        "df_task3 = pd.DataFrame(d)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "task3 = df_task3.to_dict(orient=\"records\")\n",
        "with open(\"task3-dataset.jsonl\", \"w\") as f:\n",
        "    for line in task3:\n",
        "        f.write(json.dumps(line) + \"\\n\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### TASK 1\n",
        "\n",
        "Input: World Model + Keys + Satisfied\n",
        "\n",
        "Target: formula (but not necessarily the one from the original dataset, as multiple formulas are possible.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SetEncoder(json.JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, set):\n",
        "            return list(obj)\n",
        "        return json.JSONEncoder.default(self, obj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_valuation(valuation):\n",
        "    # this is necessary, as jsonl can not serialize sets, so we convert them to lists here and back afterwards.\n",
        "    return [(v[0], list(v[1])) if v[0].isupper() else v for v in valuation]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompts = make_task1_inputs(df)\n",
        "target_sats = df['Satisfied'].tolist()\n",
        "# Object of type set is not JSON serializable, so we convert it to a list and convert back afterwards.\n",
        "valuations = df[\"Valuation\"].tolist()\n",
        "print(valuations[0])\n",
        "targets = df['Formula'].tolist()\n",
        "\n",
        "\n",
        "d = {'Input':prompts,\"Target-sat\": target_sats,\"Valuation\": valuations,'Target':targets}\n",
        "df_task1 = pd.DataFrame(d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[\"Valuation\"].to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_task1.iloc[0:19].to_json('Task1-20-datapoints.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "task1 = df_task1.to_dict(orient=\"records\")\n",
        "with open(\"Task1-dataset.jsonl\", \"w\") as f:\n",
        "    for line in task1:\n",
        "        f.write(json.dumps(line) + \"\\n\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### TASK 2:\n",
        "\n",
        "Input: Formula + Satisfied\n",
        "\n",
        "Target: World Model + Keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inputs = make_task2_inputs(df)\n",
        "models = df['World Model'].tolist()\n",
        "target_sats = df['Satisfied'].tolist()\n",
        "formulas = df['Formula'].tolist()\n",
        "keys = df['Keys'].tolist()\n",
        "targets = [m + \" \" + k for m, k in zip(models, keys)]\n",
        "\n",
        "d = {'Input':inputs,\"Target-sat\": target_sats, \"Formulas\": formulas, 'Target':targets}\n",
        "df_task2 = pd.DataFrame(d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_task2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "task2 = df_task2.to_dict(orient=\"records\")\n",
        "with open(\"Task2-dataset.jsonl\", \"w\") as f:\n",
        "    for line in task2:\n",
        "        f.write(json.dumps(line, cls=SetEncoder) + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(df_task2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
