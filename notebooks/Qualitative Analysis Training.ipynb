{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative Analysis Fewshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from llm_formalization.Parser import parse_LLM_output\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "from Parser import parse_LLM_output\n",
    "from evaluate_tasks import *\n",
    "import json\n",
    "from nltk.sem.logic import *\n",
    "import nltk\n",
    "from nltk.sem.logic import LogicParser, Expression\n",
    "from nltk.sem.evaluate import Valuation, Model\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir('../results/training-eval/') if f.endswith('.json')]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_files = [f for f in files if \"Falcon\" in f]\n",
    "falcon_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_files = [f for f in files if \"Llama\" in f]\n",
    "llama_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orca_files = [f for f in files if \"orca\" in f]\n",
    "orca_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wizard_files = [f for f in files if \"wizard\" in f]\n",
    "wizard_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Falcon trained on t1t2t3 49.1 \n",
    "- Falcon trained on t1 84.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_tot1t2t3_t1 = pd.read_json('../results/training-eval/' + falcon_files[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_tot1t2t3_t1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_tot1t2t3_t1.loc[falcon_tot1t2t3_t1['Correct'] == False].sample(n = 30, random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_tot1_t1 = pd.read_json('../results/training-eval/' + falcon_files[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_tot1_t1.loc[falcon_tot1_t1['Correct'] == False].sample(n = 30, random_state = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Falcon trained on task 2 -> hard t2 89.2 *  \n",
    "- Llama trained on task 2 -> hard t2 89.6    \n",
    "- orca t1t2t3 -> hard t2 89.6              \n",
    "- orca trained on task 2 -> hard t2 84.8      \n",
    "- wizard trained on task 2 -> hard t2 89.6  \n",
    "- wizard trained on task 3 -> hard t2 88.1 *  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_tot2_t2 = pd.read_json('../results/training-eval/' + falcon_files[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_tot2_t2.loc[falcon_tot2_t2['Correct'] == False].sample(n = 30, random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tot2_t2 = pd.read_json('../results/training-eval/' + llama_files[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tot2_t2.loc[llama_tot2_t2['Correct'] == False].sample(n = 30, random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orca_tot1t2t3_t2 = pd.read_json('../results/training-eval/' + orca_files[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orca_tot1t2t3_t2.loc[orca_tot1t2t3_t2['Correct'] == True].sample(n = 30, random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orca_tot2_t2 = pd.read_json('../results/training-eval/' + orca_files[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orca_tot2_t2.loc[orca_tot2_t2['Correct'] == False].sample(n = 30, random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wizard_tot2_t2 = pd.read_json('../results/training-eval/' + wizard_files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wizard_tot2_t2.loc[wizard_tot2_t2['Correct'] == True].sample(n = 30, random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wizard_tot3_t2 = pd.read_json('../results/training-eval/' + wizard_files[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wizard_tot3_t2.loc[wizard_tot3_t2['Correct'] == True].sample(n = 30, random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tot3_t3 = pd.read_json('../results/training-eval/' + llama_files[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tot3_t3.loc[llama_tot3_t3['Correct'] == False].sample(n = 30, random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orca_tot3_t3 = pd.read_json('../results/training-eval/' + orca_files[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orca_tot3_t3.loc[orca_tot3_t3['Correct'] == False].sample(n = 30, random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wizard_tot3_t3 = pd.read_json('../results/training-eval/' + wizard_files[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wizard_tot3_t3.loc[wizard_tot3_t3['Correct'] == False].sample(n = 30, random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WHY BAD RESULTS on Task 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- llama t1 - why low on t1?\n",
    "- orca t1 - why low on t1?\n",
    "- wizard t1 - why low on t1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tot1_t1 = pd.read_json('../results/training-eval/' + llama_files[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tot1_t1.loc[llama_tot1_t1['Correct'] == False].sample(n = 30, random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orca_tot1_t1 = pd.read_json('../results/training-eval/' + orca_files[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orca_tot1_t1.loc[orca_tot1_t1['Correct'] == False].sample(n = 30, random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wizard_tot1_t1 = pd.read_json('../results/training-eval/' + wizard_files[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wizard_tot1_t1.loc[wizard_tot1_t1['Correct'] == False].sample(n = 30, random_state = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalization to other tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wizard t3 -> t2 (88.1)  SEE ABOVE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what are typical outputs of other models that do not generalize well?\n",
    "- Llama t1 -> t3\n",
    "- Llama t2 -> t1\n",
    "- Llama t3 -> t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tot1_t3 = pd.read_json('../results/training-eval/' + llama_files[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tot1_t3.loc[llama_tot1_t3['Correct'] == False].sample(n = 30, random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tot2_t1 = pd.read_json('../results/training-eval/' + llama_files[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tot2_t1.loc[llama_tot2_t1['Correct'] == False].sample(n = 30, random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tot3_t2 = pd.read_json('../results/training-eval/' + llama_files[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tot3_t2.loc[llama_tot3_t2['Correct'] == True].sample(n = 30, random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning on all tasks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_tot1t2t3_t1 = pd.read_json('../results/training-eval/' + falcon_files[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_tot1t2t3_t1.loc[falcon_tot1t2t3_t1['Correct'] == True].sample(n = 30, random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_tot1t2t3_t2 = pd.read_json('../results/training-eval/' + falcon_files[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_tot1t2t3_t2.loc[falcon_tot1t2t3_t2['Correct'] == False].sample(n = 30, random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_tot1t2t3_t3 = pd.read_json('../results/training-eval/' + falcon_files[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_tot1t2t3_t3.loc[falcon_tot1t2t3_t3['Correct'] == True].sample(n = 30, random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tot1t2t3_t1 = pd.read_json('../results/training-eval/' + llama_files[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tot1t2t3_t1.loc[llama_tot1t2t3_t1['Correct'] == True].sample(n = 30, random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tot1t2t3_t2 = pd.read_json('../results/training-eval/' + llama_files[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tot1t2t3_t2.loc[llama_tot1t2t3_t2['Correct'] == True].sample(n = 30, random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tot1t2t3_t3 = pd.read_json('../results/training-eval/' + llama_files[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tot1t2t3_t3.loc[llama_tot1t2t3_t3['Correct'] == False].sample(n = 30, random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orca_tot1t2t3_t1 = pd.read_json('../results/training-eval/' + orca_files[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orca_tot1t2t3_t1.loc[orca_tot1t2t3_t1['Correct'] == False].sample(n = 30, random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orca_tot1t2t3_t2 = pd.read_json('../results/training-eval/' + orca_files[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orca_tot1t2t3_t2.loc[orca_tot1t2t3_t2['Correct'] == True].sample(n = 30, random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orca_tot1t2t3_t3 = pd.read_json('../results/training-eval/' + orca_files[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orca_tot1t2t3_t3.loc[orca_tot1t2t3_t3['Correct'] == True].sample(n = 30, random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wizard_tot1t2t3_t1 = pd.read_json('../results/training-eval/' + wizard_files[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wizard_tot1t2t3_t1.loc[wizard_tot1t2t3_t1['Correct'] == True].sample(n = 30, random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wizard_tot1t2t3_t2 = pd.read_json('../results/training-eval/' + wizard_files[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wizard_tot1t2t3_t2.loc[wizard_tot1t2t3_t2['Correct'] == True].sample(n = 30, random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wizard_tot1t2t3_t3 = pd.read_json('../results/training-eval/' + wizard_files[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wizard_tot1t2t3_t3.loc[wizard_tot1t2t3_t3['Correct'] == False].sample(n = 30, random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
