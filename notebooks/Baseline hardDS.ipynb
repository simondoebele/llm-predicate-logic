{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding a baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we try to find / create a baseline for our tasks.\n",
    "\n",
    "Task 3 has a 50% chance baseline (since it is only binary classification).\n",
    "\n",
    "Task 1: Gegeben ein World Modell und Keys (aus Train Datensatz) -> nehme zufällig aus allen passenden Formeln aus Train Datensatz eine Formel f’ (passend = kommen dieselben Prädikate, Variablen etc. vor), und teste accuracy (richtig oder falsch gegeben World Model, Keys und Formula). wiederhole Für ausreichend World Modell-Keys-Formel-Kombinationen, kombiniere accuracy => das ist unsere Baseline.\n",
    "\n",
    "Task 2: Analog zu task 1: Gegeben Formel (aus Train Datensatz) -> nehme zufällig aus allen anderen passenden Datenpunkten aus Train Datensatz ein die Keys (passend = kommen dieselben Prädikate, Variablen etc. vor), generiere irgendein World model. und teste accuracy (richtig oder falsch gegeben World Model, Keys und Formula). wiederhole Für ausreichend World Modell-Keys-Formel-Kombinationen, kombiniere accuracy => das ist unsere Baseline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import collections\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.sem.logic import *\n",
    "import nltk\n",
    "from nltk.sem.logic import LogicParser, Expression\n",
    "from nltk.sem.evaluate import Valuation, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'base_pred_logic_data_hard.json'\n",
    "base_dataset = pd.read_json('../datasets/' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "base_dataset.iloc[573]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1696"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_dataset = base_dataset[base_dataset[\"Predicates\"].isin([['G']])]\n",
    "len(G_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1519"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_dataset = base_dataset[base_dataset[\"Predicates\"].isin([['F']])]\n",
    "len(F_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1602"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_dataset = base_dataset[base_dataset[\"Predicates\"].isin([['H']])]\n",
    "len(H_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9025"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_G_dataset = base_dataset[base_dataset[\"Predicates\"].isin([['F', 'G']])]\n",
    "len(F_G_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8396"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_F_G_dataset = base_dataset[base_dataset[\"Predicates\"].isin([['H', 'F', 'G']])]\n",
    "len(H_F_G_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8902"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_F_dataset = base_dataset[base_dataset[\"Predicates\"].isin([['H', 'F']])]\n",
    "len(H_F_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8860"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_G_dataset = base_dataset[base_dataset[\"Predicates\"].isin([['H', 'G']])]\n",
    "len(H_G_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [F_G_dataset, G_dataset, F_dataset, H_dataset, H_F_G_dataset, H_F_dataset, H_G_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algo Task 1:\n",
    "# split dataset according to the same predicates.\n",
    "# for each datapoint in each split:\n",
    "# take another formula and see whether it turns out as it should (sat / unsat) given the world model, keys, from this datapoint...\n",
    "# repeat this (sampling) many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_valuation_back(valuation):\n",
    "    # this is necessary, as jsonl could not serialize sets, but nltk expects sets for predicates.\n",
    "    return [(v[0], set(v[1])) if v[0].isupper() else v for v in valuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline_task1(datapoint1, datapoint2):\n",
    "    valuation = datapoint1[\"Valuation\"]\n",
    "    target = datapoint1[\"Satisfied\"]\n",
    "    formula = datapoint2[\"Formula\"]\n",
    "\n",
    "    valuation = convert_valuation_back(valuation)\n",
    "    val = Valuation(valuation)\n",
    "    dom = val.domain\n",
    "    m = nltk.sem.evaluate.Model(dom, val)\n",
    "    g = nltk.sem.Assignment(dom)\n",
    "    sat = m.evaluate(str(formula), g)\n",
    "    if sat == True:\n",
    "        prediction = \"satisfied\"\n",
    "    elif sat == False:\n",
    "        prediction = \"unsatisfied\"\n",
    "\n",
    "    return prediction==target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.500888"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_baseline_datapoints = 1000000\n",
    "currentcount = 0\n",
    "\n",
    "task1_baseline_list = []\n",
    "for df in splits:\n",
    "    while currentcount < max_baseline_datapoints:\n",
    "        datapoints = df.sample(n=2) # two distinct datapoints\n",
    "        try: # if the evaluation of the parser is undefined, we do not know whether it is true, so we skip it.\n",
    "            task1_baseline_list.append(get_baseline_task1(datapoints.iloc[0], datapoints.iloc[1]))\n",
    "            currentcount += 1\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "\n",
    "task1_baseline = sum(task1_baseline_list) / len(task1_baseline_list)\n",
    "task1_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algo Task 2:\n",
    "# split dataset according to the same predicates.\n",
    "# for each datapoint in each split:\n",
    "# take another world model and see whether it turns out as it should (sat / unsat) given the world model, keys, from this datapoint...\n",
    "# repeat this (sampling) many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline_task2(datapoint1, datapoint2):\n",
    "    valuation = datapoint2[\"Valuation\"]\n",
    "    target = datapoint1[\"Satisfied\"]\n",
    "    formula = datapoint1[\"Formula\"]\n",
    "\n",
    "    valuation = convert_valuation_back(valuation)\n",
    "    val = Valuation(valuation)\n",
    "    dom = val.domain\n",
    "    m = nltk.sem.evaluate.Model(dom, val)\n",
    "    g = nltk.sem.Assignment(dom)\n",
    "    sat = m.evaluate(str(formula), g)\n",
    "    if sat == True:\n",
    "        prediction = \"satisfied\"\n",
    "    elif sat == False:\n",
    "        prediction = \"unsatisfied\"\n",
    "\n",
    "    return prediction==target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9532"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_baseline_datapoints = 10000\n",
    "\n",
    "currentcount = 0\n",
    "pass_count = 0\n",
    "\n",
    "task2_baseline_list = []\n",
    "for df in splits:\n",
    "    while currentcount < max_baseline_datapoints:\n",
    "        datapoints = df.sample(n=2) # two distinct datapoints\n",
    "        try: # if the evaluation of the parser is undefined, we do not know whether it is true, so we skip it.\n",
    "            task2_baseline_list.append(get_baseline_task2(datapoints.iloc[0], datapoints.iloc[1]))\n",
    "            currentcount += 1\n",
    "        except:\n",
    "            pass_count += 1\n",
    "            pass\n",
    "\n",
    "task2_baseline = sum(task2_baseline_list) / len(task2_baseline_list)\n",
    "task2_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
